{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import numpy as np\n",
    "import deepbayesHF\n",
    "import deepbayesHF.optimizers as optimizers\n",
    "from deepbayesHF import PosteriorModel\n",
    "from deepbayesHF.analyzers import FGSM\n",
    "from deepbayesHF.analyzers import eps_LRP\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import subprocess\n",
    "from statistics import mode\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('../../IntegratedGradients')\n",
    "from IntegratedGradients import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist data and scale down to SCALE (trying 14x14 initially)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "SCALE = (14,14)\n",
    "if not SCALE == X_train[0].shape:\n",
    "    X_train = np.array(list(map(lambda x:cv2.resize(x,SCALE,interpolation=cv2.INTER_CUBIC),X_train)))\n",
    "X_train = np.array(list(map(lambda x:cv2.cvtColor(x,cv2.COLOR_GRAY2RGB),X_train)))\n",
    "X_train = X_train/255.\n",
    "X_train = X_train.astype(\"float32\").reshape(len(X_train), SCALE[0],SCALE[1],3)\n",
    "\n",
    "if not SCALE == X_test.shape:\n",
    "    X_test = np.array(list(map(lambda x:cv2.resize(x,SCALE,interpolation=cv2.INTER_CUBIC),X_test)))\n",
    "X_test = np.array(list(map(lambda x:cv2.cvtColor(x,cv2.COLOR_GRAY2RGB),X_test)))\n",
    "X_test = X_test/255.\n",
    "X_test = X_test.astype(\"float32\").reshape(len(X_test), SCALE[0], SCALE[1],3)\n",
    "\n",
    "# make it a binary classification task (X or not X)\n",
    "target = 8\n",
    "y_train = np.array([1 if y == target else 0 for y in y_train])\n",
    "y_test = np.array([1 if y == target else 0 for y in y_test])\n",
    "\n",
    "# filter so we get about 50% target class and 50% other\n",
    "target_idxs = [i for i in range(len(y_train)) if y_train[i] == 1]\n",
    "other_idxs = [i for i in range(len(y_train)) if y_train[i] == 0]\n",
    "\n",
    "#uncomment the shuffle for training only\n",
    "#random.shuffle(other_idxs)\n",
    "other_idxs = other_idxs[:len(target_idxs)]\n",
    "# pick len(target_idxs) samples from the other_idxs\n",
    "X_train = np.array([x for i,x in enumerate(X_train) if i in other_idxs or i in target_idxs])\n",
    "y_train = np.array([y for i,y in enumerate(y_train) if i in other_idxs or i in target_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaff98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a Bayesian model\n",
    "model_name = f'mnist{SCALE[0]}x{SCALE[1]}_32_16_binary_target{target}_coverage_rgb'\n",
    "opt = optimizers.VariationalOnlineGuassNewton()\n",
    "likelihood = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "inputs = Input(shape=X_train[0].shape)\n",
    "tmp = Flatten()(inputs)\n",
    "tmp = Dense(256,activation='relu')(tmp)\n",
    "tmp = Dense(128,activation='relu')(tmp)\n",
    "predictions = Dense(2,activation='softmax')(tmp)\n",
    "model = Model(inputs=inputs,outputs=predictions)\n",
    "\n",
    "bayes_model = opt.compile(model,loss_fn=likelihood,\n",
    "                          epochs=25, learning_rate=0.25,\n",
    "                          inflate_prior=2.0, log_file='tmp/log.txt')\n",
    "bayes_model.train(X_train,y_train,X_test,y_test)\n",
    "bayes_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba14b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our model\n",
    "model_name = f'mnist{SCALE[0]}x{SCALE[1]}_32_16_binary_target{target}_coverage_rgb'\n",
    "bayes_model = PosteriorModel(model_name)\n",
    "y_pred = bayes_model.predict(X_test,n=50)\n",
    "check_accuracy = tf.keras.metrics.Accuracy(name=\"train_acc\")\n",
    "check_accuracy(y_test,np.argmax(y_pred,axis=1))\n",
    "print()\n",
    "print('Loaded model accuracy:',f'{check_accuracy.result().numpy()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "# pick a random input for testing\n",
    "n = 0\n",
    "while True:\n",
    "    n = np.random.randint(len(y_train))\n",
    "    X = X_train[n].reshape(1,*SCALE,3).astype(float)\n",
    "    y_hat = np.argmax(bayes_model.predict(X,n=N))\n",
    "    y_class = y_hat\n",
    "    if y_hat == y_train[n]:\n",
    "        break\n",
    "        \n",
    "plt.imshow(X.reshape(*SCALE,3))\n",
    "print('Prediction:',y_class)\n",
    "input_shape = X.flatten().shape\n",
    "\n",
    "print(\"Index:\",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4febc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# naive Bayesian explanation generation\n",
    "\n",
    "exps = []\n",
    "X = X_train[n].reshape(1,*SCALE,3).astype(float)  \n",
    "for i in range(50):   \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=X_train[0].shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "    model_weights = bayes_model.sample()\n",
    "    model.set_weights(model_weights)\n",
    "    #print(model.predict(X))\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X,y_train[n].reshape(1,*y_train[n].shape),epochs=1,batch_size=1)\n",
    "    model.set_weights(model_weights)\n",
    "    #print(model.predict(X))\n",
    "\n",
    "    ig = integrated_gradients(model)\n",
    "    exp = ig.explain(X.reshape(*SCALE,3),outc=y_train[n])\n",
    "    exp = exp[:,:,0]\n",
    "    max_rel = np.max(exp)\n",
    "    #limit = 0.05*max_rel\n",
    "    #exp[exp < limit] = 0\n",
    "    exp[exp > 0] = 1\n",
    "    exps.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise result\n",
    "\n",
    "cmap = dict()\n",
    "names = []\n",
    "es = []\n",
    "for e in exps:\n",
    "    if not str(e) in names:\n",
    "        names.append(str(e))\n",
    "        es.append(e)\n",
    "        cmap[names.index(str(e))] = 0\n",
    "    cmap[names.index(str(e))] += 1\n",
    "\n",
    "res = max(cmap,key=cmap.get)\n",
    "res_image = es[res]\n",
    "plt.imshow(res_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "cov = (cmap[res]/50)*100\n",
    "\n",
    "print(\"P_cover:\",cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d021bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memo import memo\n",
    "\n",
    "@memo\n",
    "def generate_min_exps(expl,threshold):\n",
    "    exps = []\n",
    "    for i in range(len(expl)):\n",
    "        orig_expl = expl\n",
    "        if orig_expl[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == len(expl) - 1:\n",
    "                s = sum(orig_expl[:i])\n",
    "            else:\n",
    "                s = sum(orig_expl[:i])+sum(orig_expl[i+1:])\n",
    "            if s < threshold:\n",
    "                exps.append(expl)\n",
    "                break\n",
    "            else:\n",
    "                new_expl = tuple(orig_expl[:i]) + (0,)\n",
    "                if i < len(expl)-1:\n",
    "                    new_expl = new_expl + tuple(orig_expl[i+1:])\n",
    "                new_exps = generate_min_exps(new_expl,threshold)\n",
    "                exps += new_exps\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# smarter method of generating covering explanation\n",
    "\n",
    "exps = []\n",
    "X = X_train[n].reshape(1,*SCALE,3).astype(float)\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=X_train[0].shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "    model_weights = bayes_model.sample()\n",
    "    model.set_weights(model_weights)\n",
    "    #print(model.predict(X))\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X,y_train[n].reshape(1,*y_train[n].shape),epochs=1,batch_size=1)\n",
    "    model.set_weights(model_weights)\n",
    "    #print(model.predict(X))\n",
    "\n",
    "    ig = integrated_gradients(model)\n",
    "    exp = ig.explain(X.reshape(*SCALE,3),outc=y_train[n])\n",
    "    exp = exp[:,:,0]\n",
    "    max_rel = np.max(exp)\n",
    "    #th = max(np.abs(np.min(ex)), np.abs(np.max(ex)))\n",
    "    #plt.imshow(ex[:,:,0], cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
    "    #plt.show()\n",
    "    limit = 0.1*max_rel\n",
    "    #limit = 0\n",
    "    exp[exp < limit] = 0\n",
    "    exp[exp > 0] = 1\n",
    "    #plt.imshow(exp)\n",
    "    #plt.show()\n",
    "    exp_list = list(set(generate_min_exps(tuple(exp.flatten()),0.9*np.sum(exp))))\n",
    "    exps += exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c55b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get covering explanation and probability\n",
    "cmap = dict()\n",
    "names = []\n",
    "es = []\n",
    "print(len(exps))\n",
    "c = 0\n",
    "for e in exps:\n",
    "    if c < 10:\n",
    "        plt.imshow(np.array(e).reshape(*SCALE))\n",
    "        plt.show()\n",
    "    c += 1\n",
    "    if not str(e) in names:\n",
    "        names.append(str(e))\n",
    "        es.append(np.array(e).reshape(*SCALE))\n",
    "        cmap[names.index(str(e))] = 0\n",
    "    cmap[names.index(str(e))] += 1\n",
    "\n",
    "res = max(cmap,key=cmap.get)\n",
    "res_image = es[res]\n",
    "plt.imshow(res_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "cov = (cmap[res]/50)*100\n",
    "\n",
    "print(\"P_cover:\",cov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
