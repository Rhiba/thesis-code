{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import numpy as np\n",
    "import deepbayesHF\n",
    "import deepbayesHF.optimizers as optimizers\n",
    "from deepbayesHF import PosteriorModel\n",
    "from deepbayesHF.analyzers import FGSM\n",
    "from deepbayesHF.analyzers import eps_LRP\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import subprocess\n",
    "from statistics import mode\n",
    "import json\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "train_dir = '/home/rhiba/bayesian-ores/training_data/gtsrb/Final_Training/Images/'\n",
    "for class_dir in os.listdir(train_dir):\n",
    "    classid = int(class_dir)\n",
    "    full_path = os.path.join(train_dir,class_dir)\n",
    "    images = [x for x in os.listdir(full_path) if x.endswith('.ppm')]\n",
    "    for i in images:\n",
    "        actual_image = imageio.imread(os.path.join(full_path,i))\n",
    "        widths.append(len(actual_image[0]))\n",
    "        heights.append(len(actual_image))\n",
    "        X_train.append(actual_image)\n",
    "        y_train.append(classid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min and max widths:',min(widths),max(widths))\n",
    "print('Min and max heights:',min(heights),max(heights))\n",
    "rescale_size = (30,30)\n",
    "print('Rescaling to:',rescale_size)\n",
    "\n",
    "if not rescale_size == X_train[0].shape:\n",
    "    X_train = np.array(list(map(lambda x:cv2.resize(x,rescale_size,interpolation=cv2.INTER_CUBIC),X_train)))\n",
    "X_train = X_train/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a59d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = list(zip(X_train,y_train))\n",
    "#random.shuffle(zipped)\n",
    "X_train = list(list(zip(*zipped))[0])\n",
    "y_train = list(list(zip(*zipped))[1])\n",
    "split_point = int(0.8*len(X_train))\n",
    "X_test = X_train[split_point:]\n",
    "X_train = X_train[:split_point]\n",
    "y_test = y_train[split_point:]\n",
    "y_train = y_train[:split_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de993cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'GTSRB_{rescale_size[0]}x{rescale_size[1]}im'\n",
    "\n",
    "opt = optimizers.VariationalOnlineGuassNewton()\n",
    "likelihood = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "inputs = Input(shape=X_train[0].shape)\n",
    "tmp = Conv2D(4,3,padding='same',activation='relu')(inputs)\n",
    "tmp = MaxPooling2D()(tmp)\n",
    "tmp = Conv2D(8,3,padding='same',activation='relu')(tmp)\n",
    "tmp = MaxPooling2D()(tmp)\n",
    "tmp = Flatten()(tmp)\n",
    "tmp = Dense(128,activation='relu')(tmp)\n",
    "predictions = Dense(43,activation='softmax')(tmp)\n",
    "model = Model(inputs=inputs,outputs=predictions)\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Input(shape=X_train[0].shape))\n",
    "model.add(Conv2D(4,3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(8,3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling2D())\n",
    "#model.add(Conv2D(64,3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(43,activation='softmax'))\n",
    "'''\n",
    "\n",
    "bayes_model = opt.compile(model,loss_fn=likelihood,\n",
    "                          epochs=25, learning_rate=0.25,\n",
    "                          inflate_prior=2.0, log_file='tmp/log.txt')\n",
    "bayes_model.train(X_train,y_train,X_test,y_test)\n",
    "bayes_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba14b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~1 min to run if testing over the whole test set \n",
    "# (use subset for shorter run time but less accurate average accuracy)\n",
    "subset = len(X_test)\n",
    "\n",
    "model_name = f'GTSRB_{rescale_size[0]}x{rescale_size[1]}im'\n",
    "bayes_model = PosteriorModel(model_name)\n",
    "y_pred = bayes_model.predict(X_test[:subset],n=50)\n",
    "check_accuracy = tf.keras.metrics.Accuracy(name=\"train_acc\")\n",
    "check_accuracy(y_test[:subset],np.argmax(y_pred,axis=1))\n",
    "print()\n",
    "print('Loaded model accuracy:',f'{check_accuracy.result().numpy()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random test input\n",
    "N = 50\n",
    "n = 0\n",
    "while True:\n",
    "    \n",
    "    n = np.random.randint(0,len(y_train))\n",
    "    X = X_train[n].reshape(1,*X_train[n].shape).astype(float)\n",
    "    break\n",
    "    \n",
    "\n",
    "plt.imshow(X_train[n],vmin=0,vmax=1)\n",
    "print('Prediction:',y_hat)\n",
    "print('n:',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f86bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the LRP explanations for N sampled networks\n",
    "input_path = 'X.npy'\n",
    "np.save(input_path,X,False)\n",
    "\n",
    "if not os.path.exists(f'exps/exp{n}_lrp/'):\n",
    "    os.mkdir(f'exps/exp{n}_lrp')\n",
    "\n",
    "iterations = 50\n",
    "grayscale = 'True'\n",
    "for i in range(iterations):\n",
    "    subprocess.Popen(['python3','get_exp.py',str(i),model_name,input_path,f'exps/exp{n}_lrp',grayscale])\n",
    "    \n",
    "full = False \n",
    "while not full:\n",
    "    if len([name for name in os.listdir(f'exps/exp{n}_lrp') if os.path.isfile(os.path.join(f'exps/exp{n}_lrp', name))]) == iterations:\n",
    "        full = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd5620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# naive method of generating bayesian explanation\n",
    "import ast\n",
    "\n",
    "all_exps = []\n",
    "for f in os.listdir(f'exps/exp{ns}_lrp'):\n",
    "    if os.path.isfile(os.path.join(f'exps/exp{ns}_lrp',f)):\n",
    "        tmp = np.load(os.path.join(f'exps/exp{ns}_lrp',f))\n",
    "        all_exps.append(tmp)\n",
    "\n",
    "net_count = len(all_exps)\n",
    "#print(net_count)\n",
    "coverage_map = dict()\n",
    "max_rel = np.max(all_exps)\n",
    "limit = 0.15*max_rel\n",
    "for exp in all_exps:\n",
    "    exp[exp < limit] = 0\n",
    "    exp[exp > 0] = 1\n",
    "ns_exps.append(all_exps)\n",
    "\n",
    "# visualise result\n",
    "cmap = dict()\n",
    "names = []\n",
    "es = []\n",
    "for e in all_exps:\n",
    "    if not str(e) in names:\n",
    "        names.append(str(e))\n",
    "        es.append(e)\n",
    "        cmap[names.index(str(e))] = 0\n",
    "    cmap[names.index(str(e))] += 1\n",
    "\n",
    "res = max(cmap,key=cmap.get)\n",
    "res_image = es[res]\n",
    "plt.imshow(res_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "cov = (cmap[res]/50)*100\n",
    "\n",
    "print(\"P_cover:\",cov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memo import memo\n",
    "\n",
    "@memo\n",
    "def generate_min_exps(expl,threshold):\n",
    "    exps = []\n",
    "    for i in range(len(expl)):\n",
    "        orig_expl = expl\n",
    "        if orig_expl[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == len(expl) - 1:\n",
    "                s = sum(orig_expl[:i])\n",
    "            else:\n",
    "                s = sum(orig_expl[:i])+sum(orig_expl[i+1:])\n",
    "            if s < threshold:\n",
    "                exps.append(expl)\n",
    "                break\n",
    "            else:\n",
    "                new_expl = tuple(orig_expl[:i]) + (0,)\n",
    "                if i < len(expl)-1:\n",
    "                    new_expl = new_expl + tuple(orig_expl[i+1:])\n",
    "                new_exps = generate_min_exps(new_expl,threshold)\n",
    "                exps += new_exps\n",
    "    return exps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better method of generating bayesian covering explanation\n",
    "import ast\n",
    "exps = dict()\n",
    "all_exps = []\n",
    "for f in os.listdir(f'exps/exp{ns}_lrp'):\n",
    "    if os.path.isfile(os.path.join(f'exps/exp{ns}_lrp',f)):\n",
    "        tmp = np.load(os.path.join(f'exps/exp{ns}_lrp',f))\n",
    "        all_exps.append(tmp)\n",
    "\n",
    "net_count = len(all_exps)\n",
    "#print(net_count)\n",
    "coverage_map = dict()\n",
    "max_rel = np.max(all_exps)\n",
    "limit = 0.25*max_rel\n",
    "all_new_exps = []\n",
    "for exp in all_exps[:5]:\n",
    "    print(exp.shape)\n",
    "    exp[exp < limit] = 0\n",
    "    exp[exp > 0] = 1\n",
    "    exp_list = list(set(generate_min_exps(tuple(exp.flatten()),0.98*np.sum(exp))))\n",
    "    all_new_exps += exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5111535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise result\n",
    "\n",
    "cmap = dict()\n",
    "pic_map = dict()\n",
    "print(len(exps))\n",
    "for e in all_new_exps:\n",
    "    e = np.array(e)\n",
    "    if not str(e) in cmap.keys():\n",
    "        cmap[str(e)] = 0\n",
    "        pic_map[str(e)] = np.array(e).reshape(30,30)\n",
    "    #else:\n",
    "    #    print('dupe')\n",
    "    cmap[str(e)] += 1\n",
    "\n",
    "res = max(cmap,key=cmap.get)\n",
    "res_image = pic_map[res]\n",
    "plt.imshow(res_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "cov = (cmap[res]/50)*100\n",
    "\n",
    "print(\"P_cover:\",cov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
